{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext is one of the two entry points for Spark functionality.\n",
    "\n",
    "# A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "# Alternatively, we could use SparkSession:\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession\\\n",
    "#        .builder\\\n",
    "#        .appName(\"Tutorial1\")\\\n",
    "#        .getOrCreate()\n",
    "\n",
    "# In order to work with RDDs we would have access to SparkContext through the SparkSession:\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuts down current context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an RDD from a list.\n",
    "\n",
    "rdd = sc.parallelize([2, 3, 4, 5, 6, 7], 3)\n",
    "rdd #showing the type\n",
    "\n",
    "# One important parameter for parallel collections is the number of partitions to cut the dataset into.\n",
    "# Spark will run one task for each partition of the cluster.\n",
    "# Typically you want 2-4 partitions for each CPU in your cluster.\n",
    "# Normally, Spark tries to set the number of partitions automatically based on your cluster.\n",
    "# However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a book'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "# Text file RDDs can be created using SparkContext’s textFile() method.\n",
    "# This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines.\n",
    "\n",
    "rdd = sc.textFile(\"book.txt\")       # textFile(): Reads a text file and return it as an RDD of Strings. \n",
    "#rdd.collect()                       # collect(): returns a list that contains all of the elements in this RDD.\n",
    "#rdd.foreach(print)                  # foreach(): applies a function to all elements of this RDD.\n",
    "#rdd.count()                         # count(): returns the number of elements in this RDD.\n",
    "#rdd.first()                         # first(): return the first element of this RDD.\n",
    "\n",
    "\n",
    "#Note: foreach(print) should be used only in local mode (when we are using only one machine).  In cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a nice book\n",
      "One book\n",
      "This is a book\n",
      "A book\n",
      "This is a bad book\n",
      "A bad book\n"
     ]
    }
   ],
   "source": [
    "#Examples on how to load more than one file to the same rdd\n",
    "\n",
    "#rdd = sc.textFile(\"book.txt,hello.txt\") #note: no space after the comma\n",
    "#rdd = sc.textFile(\"data\")\n",
    "rdd = sc.textFile(\"data/book*.txt\")\n",
    "rdd.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "6 \n",
      "7 \n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
    "\n",
    "list = rdd.collect()        # collect(): returns a list that contains all of the elements in this RDD.\n",
    "#list\n",
    "\n",
    "#Another way of showing the elements of a list\n",
    "#for num in list:\n",
    "#    print('%i ' % (num))\n",
    "\n",
    "# We should use the collect() on a smaller dataset, usually after filter(), group(), etc. \n",
    "# Retrieving on larger datasets results in out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take(n): returns a list with the first n elements of the RDD\n",
    "\n",
    "rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
    "\n",
    "#rdd.take(1)\n",
    "rdd.take(2)\n",
    "#rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map(): returns a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "rdd1 = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
    "\n",
    "rdd1.map(lambda x: x * x).collect()\n",
    "\n",
    "# Alternative\n",
    "#rdd2 = rdd1.map(lambda x: x * x)\n",
    "#rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 92, 93]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter(): returns a new RDD containing only the elements that satisfy a condition.\n",
    "\n",
    "rdd1 = sc.parallelize(range(100))\n",
    "\n",
    "rdd2 = rdd1.filter(lambda x: x > 90)\n",
    "rdd2.collect()\n",
    "\n",
    "rdd1.filter(lambda x: x > 90).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a book']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lines with some word\n",
    "\n",
    "rdd1 = sc.textFile(\"book.txt\")\n",
    "\n",
    "# What is the difference between the two alternatives below?\n",
    "#rdd2 = rdd1.filter(lambda line: \"This\" in line)\n",
    "rdd2 = rdd1.filter(lambda line: \"This\" in line.split())\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a book']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use our own functions\n",
    "\n",
    "# Lines with some word\n",
    "\n",
    "rdd1 = sc.textFile(\"book.txt\")\n",
    "\n",
    "def wordInLine(word, line):\n",
    "     return word in line.split() # returns True if word is in line, False otherwise\n",
    "\n",
    "rdd2 = rdd1.filter(lambda line: wordInLine(\"This\", line))\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of lines with some word\n",
    "\n",
    "rdd = sc.textFile(\"book.txt\")\n",
    "\n",
    "rdd.filter(lambda line: \"This\" in line.split()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce(): reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
    "\n",
    "rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
    "\n",
    "#Adding all the elements in the RDD\n",
    "\n",
    "# Alternative 1\n",
    "#rdd.reduce(lambda a, b: a + b)\n",
    "\n",
    "# Alternative 2\n",
    "from operator import add\n",
    "rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding up the sizes of all the lines of a text file using the map and reduce operations:\n",
    "\n",
    "rdd1 = sc.textFile(\"book.txt\")\n",
    "\n",
    "rdd2 = rdd1.map(lambda line: len(line))\n",
    "#rdd2.collect()\n",
    "\n",
    "rdd2.reduce(lambda a, b: a + b)\n",
    "\n",
    "# Transformations laziness explained:\n",
    "# - The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file.\n",
    "# - The second line defines rdd2 as the result of a map transformation. Again, rdd2 is not immediately computed, due to laziness.\n",
    "# - Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program. If laziness were not applied, all the intermediate results would have been returned to the driver program.\n",
    "\n",
    "# In one line...\n",
    "#rdd1.map(lambda line: len(line)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words of line with more words\n",
    "\n",
    "rdd = sc.textFile(\"book.txt\")\n",
    "\n",
    "rdd1 = rdd.map(lambda line: len(line.split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T', 'The'),\n",
       " ('q', 'quick'),\n",
       " ('b', 'brown'),\n",
       " ('f', 'fox'),\n",
       " ('j', 'jumps'),\n",
       " ('o', 'over'),\n",
       " ('t', 'the'),\n",
       " ('l', 'lazy'),\n",
       " ('d', 'dog')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just another example...\n",
    "\n",
    "rdd1 = sc.parallelize(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'])\n",
    "#rdd1.collect()\n",
    "rdd2 = rdd1.map(lambda word: (word[0], word))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [3, 4], [6, 7]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#glom(): returns an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "rdd = sc.parallelize([0, 2, 3, 4, 6, 7], 3)\n",
    "#rdd.collect()\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing the maximum value\n",
    "\n",
    "rdd = sc.parallelize([50.0, 40.0, 40.0, 70.0], 2)  \n",
    "#rdd.glom().collect()\n",
    "\n",
    "# Alternative 1\n",
    "rdd.reduce(max)\n",
    "\n",
    "# Although it works, there will be lot of shuffles between partitions for comparisons. That is not good, particularly for large data.\n",
    "\n",
    "# Rather than comparing all the values, we can\n",
    "#   1. First find maximum in each partition\n",
    "#   2. Compare maximum value between partitions to get the final max value\n",
    "\n",
    "# This can be easily done using glom as follows:\n",
    "\n",
    "# Alternative 2\n",
    "\n",
    "rdd2 = rdd.glom().map(lambda v: max(v))\n",
    "rdd2.collect()\n",
    "rdd2.reduce(max)\n",
    "\n",
    "# In one line...\n",
    "#rdd.glom().map(lambda v: max(v)).reduce(max)\n",
    "\n",
    "# Source: https://blog.madhukaraphatak.com/glom-in-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching and persisting\n",
    "\n",
    "# Adding up the sizes of all the lines of a text file using the map and reduce operations:\n",
    "\n",
    "rdd1 = sc.textFile(\"book.txt\")\n",
    "\n",
    "rdd1.persist() #rdd1.cache()\n",
    "\n",
    "rdd2 = rdd1.map(lambda line: len(line))\n",
    "\n",
    "rdd2.reduce(lambda a, b: a + b)\n",
    "\n",
    "# Spark’s RDDs are by default recomputed each time we run an action on them. If we want to reuse an RDD in multiple actions, we can ask Spark to persist it using RDD.persist(). After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in our cluster), and reuse them in future actions.\n",
    "\n",
    "# RDD.persist() method is used to store this RDD to the user-defined storage level.\n",
    "# RDD.cache() persists this RDD with the default storage level (MEMORY_ONLY).\n",
    "\n",
    "# The two functions are also available for DataFrames (covered in tutorial2_DataFrames.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count Example Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('is', 1), ('book', 2), ('This', 1), ('a', 1), ('A', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count - complete solution\n",
    "\n",
    "lines = sc.textFile(\"book.txt\")\n",
    "\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Word count ignoring case\n",
    "#counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.saveAsTextFile(\"word_count\")\n",
    "\n",
    "counts.collect()\n",
    "\n",
    "# saveAsTextFile(path) -> The path is considered as a directory, and multiple outputs will be produced in that directory.\n",
    "\n",
    "# Alternative:\n",
    "#from operator import add\n",
    "#counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'book', 'A', 'book']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count steps explained - Step 1: computing an RDD with all the words of the text file\n",
    "\n",
    "lines = sc.textFile(\"book.txt\")\n",
    "\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()\n",
    "\n",
    "# Just for curiosity, in order to see the difference between map() and flatMap():\n",
    "#words = lines.map(lambda line: line.split(\" \"))\n",
    "#words.collect()\n",
    "\n",
    "# flatMap() transformation flattens the RDD/DataFrame/Dataset after applying the function on every element and returns a new transformed Dataset. The returned Dataset will return more rows than the current DataFrame. It is also referred to as a one-to-many transformation function. This is one of the major differences between flatMap() and map()\n",
    "\n",
    "# In this case, flatMap() is used because applying line.split(\" \") to each line (element) may produce more than one output (a list with all the words in each line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 1), ('is', 1), ('a', 1), ('book', 1), ('A', 1), ('book', 1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count steps explained - Step 2: computing an RDD with a tuple (word, 1) per word\n",
    " \n",
    "lines = sc.textFile(\"book.txt\")\n",
    "\n",
    "single_word_pairs = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
    "\n",
    "single_word_pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 1), ('book', 2), ('This', 1), ('a', 1), ('A', 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count steps explained - Step 3: finally, merging (adding, in this case) the pairs resulting from the map operation...\n",
    "\n",
    "text_file = sc.textFile(\"book.txt\")\n",
    "\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.collect()\n",
    "\n",
    "# reduceByKey(): merges the values for each key using an associative and commutative reduce function.\n",
    "# This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count Example End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.146000\n"
     ]
    }
   ],
   "source": [
    "# Computing PI: This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate.\n",
    "\n",
    "import random\n",
    "\n",
    "NUM_SAMPLES = 10000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random() * 2 - 1, random.random() * 2 - 1\n",
    "    return x * x + y * y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds\n",
    "- https://spark.apache.org/docs/3.1.3/api/python/\n",
    "- Learning Spark Lightning-Fast Big Data Analysis, Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia. O'Reilley, 2015.\n",
    "- https://sparkbyexamples.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
