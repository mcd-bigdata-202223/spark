{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/17 21:30:43 WARN Utils: Your hostname, bigdata-vmware resolves to a loopback address: 127.0.1.1; using 192.168.10.135 instead (on interface ens33)\n",
      "22/11/17 21:30:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/17 21:30:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Tutorial2_DataFrames\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuts down spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = spark.read.json(\"data/people.json\")\n",
    "df = spark.read.csv(\"data/people.csv\", header=True, inferSchema=True)\n",
    "#df = spark.read.parquet(\"data/users.parquet\")\n",
    "\n",
    "#df.printSchema()\n",
    "#df.show()\n",
    "#df.show(3)\n",
    "#df.collect()\n",
    "#df.first()\n",
    "df.count()\n",
    "\n",
    "#df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to load data from hdfs:\n",
    "# 1) You should exit VS Code. Why? Because python uses the same port (9000) as hdfs.\n",
    "# 2) You should start hdfs and yarn and copy your data to hdfs.\n",
    "# 3) Reopen VS Code.\n",
    "# 4) Open file /home/bigdata/hadoop-3.4.4/etc/hadoop/core-site.xml and copy the value of property \"fs.defaultFS\", which represents the Hadoop name node path. In our case, it should have value \"hdfs://localhost:9000\".\n",
    "# 5) Start the spark session (first code cell of this notebook).\n",
    "# 6) Run this code cell.\n",
    "\n",
    "#df = spark.read.text(\"hdfs://ocalhlost:9000/user/bigdata/data/book.txt\")\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/user/bigdata/data/people.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+\n",
      "|   Name|age|           job|\n",
      "+-------+---+--------------+\n",
      "|  Maria| 25|       Manager|\n",
      "|   João| 30|Data Scientist|\n",
      "|  Jorge| 30|     Developer|\n",
      "|   Rita| 28|Data Scientist|\n",
      "|António| 32|     Developer|\n",
      "+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [['Maria', 25, 'Manager'],\n",
    "        ['João', 30, 'Data Scientist'],\n",
    "        ['Jorge', 30, 'Developer'],\n",
    "        ['Rita', 28, 'Data Scientist'],\n",
    "        ['António', 32, 'Developer']]\n",
    "\n",
    "columns = ['Name', 'age', 'job']\n",
    "  \n",
    "dataframe = spark.createDataFrame(rows, columns)\n",
    "  \n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Jorge| 30|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" and \"age\" columns\n",
    "\n",
    "df1 = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df2 = df1.select(\"name\", \"age\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------------+\n",
      "| name|(age + 1)|           job|\n",
      "+-----+---------+--------------+\n",
      "|Maria|     26.0|       Manager|\n",
      "| João|     31.0|Data Scientist|\n",
      "|Jorge|     31.0|     Developer|\n",
      "| Rita|     -1.0|Data Scientist|\n",
      "|  Bob|     33.0|     Developer|\n",
      "+-----+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the columns, but increment the age by 1\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df.select(\"name\", df.age + 1, \"job\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 5|\n",
      "|   mean|              23.0|\n",
      "| stddev|14.212670403551895|\n",
      "|    min|                -2|\n",
      "|    25%|              25.0|\n",
      "|    50%|              30.0|\n",
      "|    75%|              30.0|\n",
      "|    max|                32|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/people.csv\", header=True, inferSchema=False)\n",
    "\n",
    "#df.summary().show()\n",
    "\n",
    "#Showing statitics for the age column only\n",
    "df.select(\"age\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "30\n",
      "25\n",
      "32\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "# Getting the (distinct) values of some column\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "\n",
    "age_distinct_values = df.select('age').distinct().collect()\n",
    "\n",
    "for row in age_distinct_values:\n",
    "    print(row[0])\n",
    "\n",
    "#This shows all values\n",
    "#from pyspark.sql.functions import *\n",
    "#all_values = df.select(collect_list(\"age\")).first()[0]\n",
    "#print(all_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 30|    2|\n",
      "| 25|    1|\n",
      "| 32|    1|\n",
      "| -2|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df = df.groupBy(\"age\").count()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# groupBy() returns a GroupData, wich consists in set of methods for aggregations on a DataFrame, created by DataFrame.groupBy().\\n\",\n",
    "# The count() method of GroupData counts the number of records for each group and returns a DataFrame.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "+-----+------+-----+\n",
      "| name|salary|prize|\n",
      "+-----+------+-----+\n",
      "|Maria|  2000|  100|\n",
      "| João|  1500|  250|\n",
      "|Jorge|  1400|  200|\n",
      "| Rita|  1600|  500|\n",
      "|  Bob|  1000|   -1|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove columns\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "\n",
    "#df2 = df.drop(\"age\")\n",
    "df2 = df.drop(\"age\", \"job\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------+------+-----+\n",
      "| name| age|           job|salary|prize|\n",
      "+-----+----+--------------+------+-----+\n",
      "|Maria|null|       Manager|  2000|  100|\n",
      "|Maria|  25|       Manager|  null|  100|\n",
      "| null|null|          null|  null| null|\n",
      "| João|  30|Data Scientist|  1500|  250|\n",
      "|Jorge|  30|     Developer|  1400|  200|\n",
      "| Rita|  -2|          null|  1600|  500|\n",
      "|  Bob|  32|     Developer|  1000|   -1|\n",
      "| null|  32|     Developer|  1000|   -1|\n",
      "+-----+----+--------------+------+-----+\n",
      "\n",
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  null|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "| null| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with NULL values\n",
    "\n",
    "#Note: in order to try this, remove some value(s) in the dataset\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True) \n",
    "df.show()\n",
    "\n",
    "#df2 = df.dropna() #the same as df.dropna(\"any\")\n",
    "#df2 = df.na.drop() #the same df2 = df.dropna()\n",
    "#df2 = df.dropna(\"all\") #removes rows with null values in all columns\n",
    "df2 = df.dropna(subset=[\"age\",\"job\"]) #removes rows with null values in the given columns\n",
    "\n",
    "df2.show()\n",
    "\n",
    "#-------------------\n",
    "\n",
    "#drop(how='any', thresh=None, subset=None)\n",
    "\n",
    "#All these parameters are optional.\n",
    "\n",
    "#how – This takes values ‘any’ or ‘all’. By using ‘any’, drop a row if it contains NULLs on any columns. By using ‘all’, drop a row only if all columns have NULL values. Default is ‘any’.\n",
    "#thresh – This takes int value, Drop rows that have less than threshhold non-null values. Default is ‘None’.\n",
    "#subset – Use this to select the columns for NULL values. Default is ‘None.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "7\n",
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "print(df.count())\n",
    "\n",
    "df2 = df.distinct()\n",
    "df2.show()\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "7\n",
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Remove rows that have the same combination of values accross multiple selected columns\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "print(df.count())\n",
    "\n",
    "df2 = df.dropDuplicates([\"age\", \"job\"])\n",
    "df2.show()\n",
    "print(df2.count())\n",
    "\n",
    "#Note\n",
    "# df2 = df.dropDuplicates() is the same as df2 = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "+----+---+--------------+------+-----+\n",
      "|name|age|           job|salary|prize|\n",
      "+----+---+--------------+------+-----+\n",
      "|João| 30|Data Scientist|  1500|  250|\n",
      "+----+---+--------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select rows based on a condition\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "\n",
    "df2 = df.filter(df.age > 27)\n",
    "\n",
    "# Alternative:\n",
    "#df2 = df[df.age >= 27]\n",
    "\n",
    "# With condition involving more than one column\n",
    "df2 = df[(df.age > 27) & (df.job == 'Data Scientist')]\n",
    "\n",
    "df2.show()\n",
    "\n",
    "# We could also have a condition that compares the values of two columns\n",
    "#df2 = df.filter(df.col1 > df.col2)\n",
    "\n",
    "#Exercise: How can you remove all the rows with some specific value in a column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "Row(name=5, age=4, job=3, salary=5, prize=5)\n",
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "+---+--------------+\n",
      "|age|           job|\n",
      "+---+--------------+\n",
      "| 25|       Manager|\n",
      "| 30|Data Scientist|\n",
      "| 30|     Developer|\n",
      "| -2|Data Scientist|\n",
      "| 32|     Developer|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove columns with the same value for all rows and columns with different values for all rows\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "\n",
    "cnt = df.agg(*(f.countDistinct(c).alias(c) for c in df.columns)).first()\n",
    "print(cnt)\n",
    "\n",
    "# drop columns with the same value for all rows\n",
    "df.drop(*(c for c in cnt.asDict() if cnt[c] == 1)).show()\n",
    "\n",
    "# drop columns with different values for all rows\n",
    "df.drop(*(c for c in cnt.asDict() if cnt[c] == df.count())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|   -1|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n",
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| João| 30|Data Scientist|  1500|  250|\n",
      "|Jorge| 30|     Developer|  1400|  200|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "|  Bob| 32|     Developer|  1000|    0|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating/changing values\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "df.show()\n",
    "#df.printSchema()\n",
    "\n",
    "#df2 = df.withColumn(\"Yearly Income\", df.salary * 14 + df.prize)\n",
    "\n",
    "#df2 = df.withColumn(\"salary\", df.salary.cast(IntegerType()))\n",
    "\n",
    "#df2 = df.withColumn(\"job\", \n",
    "#            when(df.job == \"Developer\",\"Programmer\").\n",
    "#            when(df.job == \"Manager\",\"Project Manager\").\n",
    "#            otherwise(df.job))\n",
    "\n",
    "df2 = df.withColumn(\"prize\", when(df.prize < 0, 0).otherwise(df.prize))\n",
    "\n",
    "df2.show()\n",
    "#df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------+-----+\n",
      "| name|age|           job|salary|prize|\n",
      "+-----+---+--------------+------+-----+\n",
      "|Maria| 25|       Manager|  2000|  100|\n",
      "| Rita| -2|Data Scientist|  1600|  500|\n",
      "+-----+---+--------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "vintagesDF = spark.sql(\"SELECT * FROM people WHERE age < 30\")\n",
    "vintagesDF.show()\n",
    "\n",
    "# See also createGlobalTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run SQL on files directly\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM parquet.`data/users.parquet`\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a data frame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Maria| 25|\n",
      "| João| 30|\n",
      "|Jorge| 30|\n",
      "| Rita| -2|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" and \"age\" columns\n",
    "\n",
    "df1 = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df2 = df1.select(\"name\", \"age\")\n",
    "\n",
    "df2.show()\n",
    "\n",
    "print(df2.rdd.getNumPartitions())\n",
    "\n",
    "df2.write.csv(\"data/people_name_age\", header=True)\n",
    "\n",
    "# Alternative\n",
    "#df2.write.format(\"csv\").save(\"data/people_name_age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|    hello world|\n",
      "|      Hello spa|\n",
      "|    hello spark|\n",
      "|    spark hello|\n",
      "|hellosparkhello|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='hello world'),\n",
       " Row(value='Hello spa'),\n",
       " Row(value='hello spark'),\n",
       " Row(value='spark hello'),\n",
       " Row(value='hellosparkhello')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|    hello spark|\n",
      "|    spark hello|\n",
      "|hellosparkhello|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lines containing some word\n",
    "\n",
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "linesWithSpark = df.filter(df.value.contains(\"spark\"))\n",
    "\n",
    "linesWithSpark.show()\n",
    "\n",
    "df.filter(df.value.contains(\"spark\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum row size\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "#df.select(size(split(df.value, \"\\s+\")) \\\n",
    "#        .name(\"numWords\")) \\\n",
    "#        .agg(max(col(\"numWords\"))) \\\n",
    "#        .collect()\n",
    "\n",
    "#df.select(split(df.value, \"\\s+\").alias(\"wordlist\")).show()\n",
    "\n",
    "#df.select(size(split(df.value, \"\\s+\"))).show()\n",
    "\n",
    "#df.select(size(split(df.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).first().__getitem__(\"max(numWords)\")\n",
    "\n",
    "df.select(size(split(df.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\")).name(\"max\")).first().__getitem__(\"max\")\n",
    "\n",
    "# This first line maps a line to an integer value and aliases it as “numWords”, creating a new DataFrame. agg is called on that DataFrame to find the largest word count. The arguments to select and agg are both Column, we can use df.colName to get a column from a DataFrame. We can also import pyspark.sql.functions, which provides a lot of convenient functions to build a new Column from an old one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           word|count|\n",
      "+---------------+-----+\n",
      "|          hello|    3|\n",
      "|          asdsf|    1|\n",
      "|hellosparkhello|    1|\n",
      "|          Hello|    1|\n",
      "|          spark|    2|\n",
      "|          world|    1|\n",
      "|            spa|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of occurences of each word\n",
    "\n",
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "wordCounts = df.select(explode(split(df.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "\n",
    "wordCounts.show()\n",
    "\n",
    "# explode(): returns a new row for each element in the given array or map. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "# Here, we use the explode function in select, to transform a Dataset of lines to a Dataset of words, and then combine groupBy and count to compute the per-word counts in the file as a DataFrame of 2 columns: “word” and “count”.\n",
    "\n",
    "# Notice that the above code basically implements a MapReduce flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interoperating with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|ERROR ddfdd MySQL...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data/logs.txt\")\n",
    "\n",
    "#rdd.foreach(print)\n",
    "\n",
    "# Creates a DataFrame having a single column named \"line\"\n",
    "df = rdd.map(lambda r: Row(r)).toDF([\"line\"])\n",
    "\n",
    "#df.show()\n",
    "\n",
    "#errorsDF = df.filter(col(\"line\").like(\"%ERROR%\"))\n",
    "#errorsDF = df.filter(col(\"line\").like(\"ERROR%\"))\n",
    "errorsDF = df.filter(col(\"line\").like(\"ERROR_%MySQL%\"))\n",
    "\n",
    "# Counts all the errors\n",
    "#errorsDF.count()\n",
    "\n",
    "# Counts errors mentioning MySQL\n",
    "#errorsDF.filter(col(\"line\").like(\"%MySQL%\")).count()\n",
    "\n",
    "# Fetches the MySQL errors as an array of strings\n",
    "#errorsDF.filter(col(\"line\").like(\"%MySQL%\")).collect()\n",
    "\n",
    "errorsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def schema_inference_example(spark: SparkSession, filePath) -> None:\n",
    "  \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    \n",
    "    linesRDD = sc.textFile(filePath)\n",
    "\n",
    "    #linesRDD.foreach(print)\n",
    "\n",
    "    partsRDD = linesRDD.map(lambda line: line.split(\",\"))\n",
    "\n",
    "    #partsRDD.foreach(print)\n",
    "\n",
    "    peopleRDD = partsRDD.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    #peopleRDD.foreach(print)\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    \n",
    "    peopleDF = spark.createDataFrame(peopleRDD)\n",
    "    \n",
    "    #peopleDF.show()\n",
    "\n",
    "    #peopleDF.printSchema()\n",
    "\n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "    \n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "\n",
    "    almost30DF = spark.sql(\"SELECT name FROM people WHERE age >= 28 AND age < 30\")\n",
    "\n",
    "    #almost30DF.show()\n",
    "    \n",
    "    # rdd (below) returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    # That is, it returns the content of the DataFrame as a pyspark.RDD of Row.\n",
    "    \n",
    "    almost30NamesRDD = almost30DF.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "    \n",
    "    #almost30NamesRDD.foreach(print)\n",
    "\n",
    "    #Showing results another way\n",
    "    #arrayAlmost30 = almost30NamesRDD.collect()\n",
    "\n",
    "    #for name in arrayAlmost30:\n",
    "    #    print(name)\n",
    "\n",
    "schema_inference_example(spark, \"data/people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Maria| 25|\n",
      "| João| 29|\n",
      "|Jorge| 30|\n",
      "| Rita| 28|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Programmatically Specifying the Schema\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "\n",
    "def programmatic_schema_example(spark: SparkSession, filePath) -> None:\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    linesRDD = sc.textFile(filePath)\n",
    "    partsRDD = linesRDD.map(lambda line: line.split(\",\"))\n",
    "    \n",
    "    # Each line is converted to a tuple.\n",
    "    peopleRDD = partsRDD.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    #peopleRDD.foreach(print)\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    \n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    peopleDF = spark.createDataFrame(peopleRDD, schema)\n",
    "    peopleDF.show()\n",
    "\n",
    "    peopleDF.printSchema()\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    #resultsDF = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    #resultsDF.show()\n",
    "\n",
    "programmatic_schema_example(spark, \"data/people.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame column methods\n",
    "\n",
    "Methods that take column names as arguments:\n",
    "- corr(col1, col2): two column names.\n",
    "- cov(col1, col2): two column names.\n",
    "- crosstab(col1, col2): two column names.\n",
    "- describe(*cols): **cols refers to only column names (strings).*\n",
    "\n",
    "Methods that take column names or column expressions or both as arguments:\n",
    "- drop(*cols): *a list of column names OR a single column expression.*\n",
    "- groupBy(*cols): column name (string) or column expression or both.\n",
    "- rollup(*cols): column name (string) or column expression or both.\n",
    "- select(*cols): column name (string) or column expression or both.\n",
    "- sort(*cols, **kwargs): column name (string) or column expression or both.\n",
    "- sortWithinPartitions(*cols, **kwargs): column name (string) or column expression or both.\n",
    "- orderBy(*cols, **kwargs): column name (string) or column expression or both.\n",
    "- sampleBy(col, fractions, sed=None): a column name.\n",
    "- toDF(*cols): a list of column names (string).\n",
    "- withColumn(colName, col): colName refers to column name; col refers to a column expression.\n",
    "- withColumnRenamed(existing, new): takes column names as arguments.\n",
    "- filter(condition): *condition refers to a column expression that returns types.BooleanType of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes\n",
    "- https://spark.apache.org/docs/latest/quick-start.html\n",
    "- https://spark.apache.org/docs/3.1.3/api/python/ \n",
    "- Learning Spark Lightning-Fast Big Data Analysis, Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia. O'Reilley, 2015.\n",
    "- https://sparkbyexamples.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
