{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 16:26:57 WARN Utils: Your hostname, bigdata-vmware resolves to a loopback address: 127.0.1.1; using 192.168.10.135 instead (on interface ens33)\n",
      "22/11/09 16:26:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 16:26:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Tutorial2_DataFrames\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuts down spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n",
      "+-----+---+--------------+\n",
      "| name|age|           job|\n",
      "+-----+---+--------------+\n",
      "|Maria| 25|       Manager|\n",
      "| João| 30|Data Scientist|\n",
      "|Jorge| 30|     Developer|\n",
      "| Rita| 28|Data Scientist|\n",
      "|  Bob| 32|     Developer|\n",
      "+-----+---+--------------+\n",
      "\n",
      "+-----+---+--------------+\n",
      "| name|age|           job|\n",
      "+-----+---+--------------+\n",
      "|Maria| 25|       Manager|\n",
      "| João| 30|Data Scientist|\n",
      "|Jorge| 30|     Developer|\n",
      "+-----+---+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Maria', age='25', job='Manager'),\n",
       " Row(name='João', age='30', job='Data Scientist'),\n",
       " Row(name='Jorge', age='30', job='Developer'),\n",
       " Row(name='Rita', age='28', job='Data Scientist'),\n",
       " Row(name='Bob', age='32', job='Developer')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = spark.read.json(\"data/people.json\")\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "#df = spark.read.parquet(\"data/users.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.show(3)\n",
    "df.collect()\n",
    "#df.first()\n",
    "#df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to load data from hdfs:\n",
    "# 1) You should exit VS Code. Why? Because python uses the same port (9000) as hdfs.\n",
    "# 2) You should start hdfs and yarn and copy your data to hdfs.\n",
    "# 3) Reopen VS Code.\n",
    "# 4) Open file /home/bigdata/hadoop-3.4.4/etc/hadoop/core-site.xml and copy the value of property \"fs.defaultFS\", which represents the Hadoop name node path. In our case, it should have value \"hdfs://localhost:9000\".\n",
    "# 4) Start the spark session (first code cell of this notebook).\n",
    "# 5) Run this code cell.\n",
    "\n",
    "#df = spark.read.text(\"hdfs://localhost:9000/user/bigdata/data/book.txt\")\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/user/bigdata/data/people.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------+\n",
      "|   Name|age|           job|\n",
      "+-------+---+--------------+\n",
      "|  Maria| 25|       Manager|\n",
      "|   João| 30|Data Scientist|\n",
      "|  Jorge| 30|     Developer|\n",
      "|   Rita| 28|Data Scientist|\n",
      "|António| 32|     Developer|\n",
      "+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [['Maria', 25, 'Manager'],\n",
    "        ['João', 30, 'Data Scientist'],\n",
    "        ['Jorge', 30, 'Developer'],\n",
    "        ['Rita', 28, 'Data Scientist'],\n",
    "        ['António', 32, 'Developer']]\n",
    "\n",
    "columns = ['Name', 'age', 'job']\n",
    "  \n",
    "dataframe = spark.createDataFrame(rows, columns)\n",
    "  \n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Jorge| 30|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" and \"age\" columns\n",
    "\n",
    "df1 = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df2 = df1.select(\"name\", \"age\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "| name|(age + 1)|      job|\n",
      "+-----+---------+---------+\n",
      "|Jorge|     31.0|Developer|\n",
      "|  Bob|     33.0|Developer|\n",
      "+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the columns, but increment the age by 1\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df.select(\"name\", df['age'] + 1, \"job\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df.filter(df['age'] > 28).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 30|    2|\n",
      "| 25|    1|\n",
      "| 32|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "\n",
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df = df.groupBy(\"age\").count().show()\n",
    "\n",
    "# groupBy() returns a GroupData, wich consists in set of methods for aggregations on a DataFrame, created by DataFrame.groupBy().\n",
    "# The count() method of GroupData counts the number of records for each group and returns a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| name|age|           job|\n",
      "+-----+---+--------------+\n",
      "|Maria| 25|       Manager|\n",
      "| Rita| 28|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df.createOrReplaceTempView(\"vintages\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM vintages WHERE age < 30\")\n",
    "sqlDF.show()\n",
    "\n",
    "# See also createGlobalTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run SQL on files directly\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM parquet.`data/users.parquet`\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a data frame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Maria| 25|\n",
      "| João| 30|\n",
      "|Jorge| 30|\n",
      "| Rita| 28|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" and \"age\" columns\n",
    "\n",
    "df1 = spark.read.csv(\"data/people.csv\", header=True)\n",
    "\n",
    "df2 = df1.select(\"name\", \"age\")\n",
    "\n",
    "df2.show()\n",
    "\n",
    "print(df2.rdd.getNumPartitions())\n",
    "\n",
    "df2.write.csv(\"data/people_name_age\", header=True)\n",
    "\n",
    "# Alternative\n",
    "#df2.write.format(\"csv\").save(\"data/people_name_age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "| hello word|\n",
      "|  Hello spa|\n",
      "|hello spark|\n",
      "|spark hello|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='hello word'),\n",
       " Row(value='Hello spa'),\n",
       " Row(value='hello spark'),\n",
       " Row(value='spark hello')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|    hello spark|\n",
      "|    spark hello|\n",
      "|hellosparkhello|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lines containing some word\n",
    "\n",
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "linesWithSpark = df.filter(df.value.contains(\"spark\"))\n",
    "\n",
    "linesWithSpark.show()\n",
    "\n",
    "df.filter(df.value.contains(\"spark\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=2)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum row size\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "df.select(size(split(df.value, \"\\s+\")) \\\n",
    "        .name(\"numWords\")) \\\n",
    "        .agg(max(col(\"numWords\"))) \\\n",
    "        .collect()\n",
    "\n",
    "#df.select(split(df.value, \"\\s+\")).show()\n",
    "\n",
    "#df.select(size(split(df.value, \"\\s+\"))).show()\n",
    "\n",
    "#df.select(size(split(df.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).first().__getitem__(\"max(numWords)\")\n",
    "\n",
    "#df.select(size(split(df.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\")).name(\"max\")).first().__getitem__(\"max(numWords)\")\n",
    "\n",
    "# This first maps a line to an integer value and aliases it as “numWords”, creating a new DataFrame. agg is called on that DataFrame to find the largest word count. The arguments to select and agg are both Column, we can use df.colName to get a column from a DataFrame. We can also import pyspark.sql.functions, which provides a lot of convenient functions to build a new Column from an old one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|hello|    3|\n",
      "|Hello|    1|\n",
      "|spark|    2|\n",
      "|world|    1|\n",
      "|  spa|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of occurences of each word\n",
    "\n",
    "df = spark.read.text(\"data/hello.txt\")\n",
    "\n",
    "wordCounts = df.select(explode(split(df.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "\n",
    "wordCounts.show()\n",
    "\n",
    "# explode(): returns a new row for each element in the given array or map. Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "\n",
    "# Here, we use the explode function in select, to transform a Dataset of lines to a Dataset of words, and then combine groupBy and count to compute the per-word counts in the file as a DataFrame of 2 columns: “word” and “count”.\n",
    "\n",
    "# Notice that the above code basically implements a MapReduce flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interoperating with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile(\"data/logs.txt\")\n",
    "\n",
    "# Creates a DataFrame having a single column named \"line\"\n",
    "df = rdd.map(lambda r: Row(r)).toDF([\"line\"])\n",
    "\n",
    "errorsDF = df.filter(col(\"line\").like(\"%ERROR%\"))\n",
    "#errorsDF = df.filter(col(\"line\").like(\"ERROR%\"))\n",
    "#errorsDF = df.filter(col(\"line\").like(\"ERROR_%MySQL%\"))\n",
    "\n",
    "# Counts all the errors\n",
    "errorsDF.count()\n",
    "\n",
    "# Counts errors mentioning MySQL\n",
    "#errorsDF.filter(col(\"line\").like(\"%MySQL%\")).count()\n",
    "\n",
    "# Fetches the MySQL errors as an array of strings\n",
    "#errorsDF.filter(col(\"line\").like(\"%MySQL%\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def schema_inference_example(spark: SparkSession, filePath) -> None:\n",
    "  \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    \n",
    "    linesRDD = sc.textFile(filePath)\n",
    "\n",
    "    #linesRDD.foreach(print)\n",
    "\n",
    "    partsRDD = linesRDD.map(lambda line: line.split(\",\"))\n",
    "\n",
    "    #partsRDD.foreach(print)\n",
    "\n",
    "    peopleRDD = partsRDD.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    #peopleRDD.foreach(print)\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    \n",
    "    peopleDF = spark.createDataFrame(peopleRDD)\n",
    "    \n",
    "    #peopleDF.show()\n",
    "\n",
    "    #peopleDF.printSchema()\n",
    "\n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "    \n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "\n",
    "    almost30DF = spark.sql(\"SELECT name FROM people WHERE age >= 28 AND age < 30\")\n",
    "\n",
    "    #almost30DF.show()\n",
    "    \n",
    "    # rdd (below) returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    # That is, it returns the content of the DataFrame as a pyspark.RDD of Row.\n",
    "    \n",
    "    almost30NamesRDD = almost30DF.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "    \n",
    "    #almost30DF.foreach(print)\n",
    "\n",
    "    #arrayAlmost30 = almost30NamesRDD.collect()\n",
    "\n",
    "    #for name in arrayAlmost30:\n",
    "    #    print(name)\n",
    "\n",
    "schema_inference_example(spark, \"data/people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Maria|\n",
      "| João|\n",
      "|Jorge|\n",
      "| Rita|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Programmatically Specifying the Schema\n",
    "\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "def programmatic_schema_example(spark: SparkSession, filePath) -> None:\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    linesRDD = sc.textFile(filePath)\n",
    "    partsRDD = linesRDD.map(lambda line: line.split(\",\"))\n",
    "    \n",
    "    # Each line is converted to a tuple.\n",
    "    peopleRDD = partsRDD.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    #peopleRDD.foreach(print)\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    \n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    peopleDF = spark.createDataFrame(peopleRDD, schema)\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    resultsDF = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    resultsDF.show()\n",
    "\n",
    "\n",
    "programmatic_schema_example(spark, \"data/people.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes\n",
    "- https://spark.apache.org/docs/latest/quick-start.html\n",
    "- https://spark.apache.org/docs/3.1.3/api/python/ \n",
    "- Learning Spark Lightning-Fast Big Data Analysis, Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia. O'Reilley, 2015.\n",
    "- https://sparkbyexamples.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
